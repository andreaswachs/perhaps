# Perhaps Helm Chart Values
# ===========================

# Global settings
global:
  # Image repository and tag
  image:
    repository: ghcr.io/perhaps-finance/perhaps
    tag: latest
    pullPolicy: IfNotPresent

  # Image pull secrets (if using private registry)
  imagePullSecrets: []

  # Pod annotations applied to all pods
  podAnnotations: {}

  # Pod security context applied to all pods
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

# Web deployment configuration
web:
  # Number of replicas
  replicaCount: 2

  # Container port
  port: 3000

  # Resource requests and limits
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "1000m"

  # Liveness probe configuration
  livenessProbe:
    httpGet:
      path: /up
      port: http
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Readiness probe configuration
  readinessProbe:
    httpGet:
      path: /up
      port: http
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  # Additional environment variables
  extraEnv: []

  # Node selector
  nodeSelector: {}

  # Tolerations
  tolerations: []

  # Affinity rules
  affinity: {}

  # Pod Disruption Budget
  pdb:
    enabled: true
    # Minimum number of pods that must be available during disruption
    minAvailable: 1
    # Or use maxUnavailable instead (only set one)
    # maxUnavailable: 1

  # Horizontal Pod Autoscaler
  hpa:
    enabled: false
    minReplicas: 2
    maxReplicas: 10
    # Target CPU utilization percentage
    targetCPUUtilizationPercentage: 70
    # Target memory utilization percentage
    targetMemoryUtilizationPercentage: 80
    # Custom metrics (optional)
    customMetrics: []
    # Scaling behavior (optional)
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 10
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Percent
            value: 100
            periodSeconds: 15
          - type: Pods
            value: 4
            periodSeconds: 15
        selectPolicy: Max

  # Enable default pod anti-affinity (spread pods across nodes)
  defaultAntiAffinity: true

# Worker deployment configuration
worker:
  # Enable worker deployment
  enabled: true

  # Number of replicas
  replicaCount: 2

  # Health check port (SidekiqHealthCheck server)
  port: 7433

  # Sidekiq concurrency (number of threads per worker)
  concurrency: 5

  # Termination grace period for graceful shutdown
  terminationGracePeriodSeconds: 60

  # Resource requests and limits
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "500m"

  # Liveness probe configuration
  livenessProbe:
    httpGet:
      path: /health
      port: health
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Readiness probe configuration
  readinessProbe:
    httpGet:
      path: /ready
      port: health
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  # Additional environment variables
  extraEnv: []

  # Node selector
  nodeSelector: {}

  # Tolerations
  tolerations: []

  # Affinity rules
  affinity: {}

  # Pod Disruption Budget
  pdb:
    enabled: true
    minAvailable: 1
    # maxUnavailable: 1

  # Horizontal Pod Autoscaler
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    customMetrics: []
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 10
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
        selectPolicy: Max

  # Enable default pod anti-affinity (spread pods across nodes)
  defaultAntiAffinity: true

  # Optional: Dedicated worker for scheduled jobs (cron)
  scheduledWorker:
    enabled: false
    replicaCount: 1
    resources:
      requests:
        memory: "128Mi"
        cpu: "50m"
      limits:
        memory: "256Mi"
        cpu: "200m"

# Service configuration
service:
  type: ClusterIP
  port: 80

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: perhaps.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
    # - secretName: perhaps-tls
    #   hosts:
    #     - perhaps.local

# Database configuration
database:
  # Run migrations on startup (only leader web pod)
  runMigrations: true

  # External PostgreSQL connection
  host: ""
  port: 5432
  name: perhaps_production
  username: perhaps
  # Password should be provided via secret
  existingSecret: ""
  existingSecretKey: "password"

# Redis configuration
redis:
  # External Redis connection
  url: ""
  # For cache (optional, uses same Redis if not specified)
  cacheUrl: ""

# Application secrets
secrets:
  # Rails secret key base
  secretKeyBase: ""
  # Use existing secret instead of creating one
  existingSecret: ""

  # Optional: Anthropic API key for AI features
  anthropicApiKey: ""

  # Optional: GoCardless credentials
  gocardlessSecretId: ""
  gocardlessSecretKey: ""

  # Optional: OpenID signing key for MCP
  openidSigningKey: ""

# Application configuration
config:
  # Rails environment
  railsEnv: production

  # Self-hosted mode
  selfHosted: "true"

  # Force SSL
  forceSSL: "true"

  # Assume SSL (behind load balancer)
  assumeSSL: "true"

  # App host for OIDC issuer
  appHost: ""

  # Additional config environment variables
  extraConfig: {}

# ServiceAccount configuration
serviceAccount:
  create: true
  annotations: {}
  name: ""
